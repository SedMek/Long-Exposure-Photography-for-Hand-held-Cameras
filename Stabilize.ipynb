{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Input and Output Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input video\n",
    "input_video = 'Data/moving_man_shaky.MOV'\n",
    "cap = cv2.VideoCapture(input_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frame count\n",
    "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Get fps\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Get width and height of video stream\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the codec for output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOOTHING_RADIUS = 100 # TODO: Try different values\n",
    "\n",
    "# Set up output video\n",
    "out = cv2.VideoWriter('{}_combined_out_{}.avi'.format(input_video.split('.')[0], SMOOTHING_RADIUS), fourcc, fps, (2*w, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the first frame and convert it to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read first frame\n",
    "_, prev = cap.read() \n",
    " \n",
    "# Convert frame to grayscale\n",
    "prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find motion between frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-define transformation-store array\n",
    "transforms = np.zeros((n_frames-1, 3), np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 368/368 [00:14<00:00, 32.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(n_frames-2)):\n",
    "# for i in tqdm(range(1)):\n",
    "    # Detect feature points in previous frame\n",
    "    prev_pts = cv2.goodFeaturesToTrack(prev_gray,\n",
    "                                       maxCorners=200,\n",
    "                                       qualityLevel=0.01,\n",
    "                                       minDistance=30,\n",
    "                                       blockSize=3)\n",
    "    for pt in prev_pts:\n",
    "        cv2.circle(prev_gray, (pt[0][0],pt[0][1]), 10,(255,255,0),-1)\n",
    "    # Read next frame\n",
    "    success, curr = cap.read() \n",
    "    if not success: \n",
    "        print(\"Finished the whole video\")\n",
    "        break\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate optical flow (i.e. track feature points)\n",
    "    curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None) \n",
    " \n",
    "    # Sanity check\n",
    "    assert prev_pts.shape == curr_pts.shape \n",
    "    \n",
    "    # Filter only valid points (the ones that are not occulated or out of frame)\n",
    "    idx = np.where(status==1)[0]\n",
    "    prev_pts = prev_pts[idx]\n",
    "    curr_pts = curr_pts[idx]\n",
    "    \n",
    "    #Find transformation matrix\n",
    "    # m = cv2.estimateRigidTransform(prev_pts, curr_pts, fullAffine=False) #will only work with OpenCV-3 or less\n",
    "    m = cv2.estimateAffinePartial2D(prev_pts, curr_pts)[0]\n",
    "    \n",
    "    # Extract traslation\n",
    "    dx = m[0,2]\n",
    "    dy = m[1,2]\n",
    "\n",
    "    # Extract rotation angle\n",
    "    da = np.arctan2(m[1,0], m[0,0])\n",
    "\n",
    "    # Store transformation\n",
    "    transforms[i] = [dx,dy,da]\n",
    "\n",
    "    cv2.imshow('image',prev_gray)\n",
    "    cv2.waitKey(0)\n",
    "    # Move to next frame\n",
    "    prev_gray = curr_gray\n",
    "    \n",
    "#     print(\"Frame: \" + str(i) +  \"/\" + str(n_frames) + \" -  Tracked points : \" + str(len(prev_pts)))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate smooth motion between frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trajectory using cumulative sum of transformations\n",
    "trajectory = np.cumsum(transforms, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingAverage(curve, radius): \n",
    "    window_size = 2 * radius + 1\n",
    "    # Define the filter \n",
    "    f = np.ones(window_size)/window_size \n",
    "    # Add padding to the boundaries \n",
    "    curve_pad = np.lib.pad(curve, (radius, radius), 'edge') \n",
    "    # Apply convolution \n",
    "    curve_smoothed = np.convolve(curve_pad, f, mode='same') \n",
    "    # Remove padding \n",
    "    curve_smoothed = curve_smoothed[radius:-radius]\n",
    "    # return smoothed curve\n",
    "    return curve_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(trajectory): \n",
    "    smoothed_trajectory = np.copy(trajectory) \n",
    "    # Filter the x, y and angle curves\n",
    "    for i in range(3):\n",
    "        smoothed_trajectory[:,i] = movingAverage(trajectory[:,i], radius=SMOOTHING_RADIUS)\n",
    "\n",
    "    return smoothed_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_trajectory = smooth(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference in smoothed_trajectory and trajectory\n",
    "difference = smoothed_trajectory - trajectory\n",
    "  \n",
    "# Calculate newer transformation array\n",
    "transforms_smooth = transforms + difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply smoothed camera motion to frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixBorder(frame):\n",
    "    s = frame.shape\n",
    "    # Scale the image 4% without moving the center\n",
    "    T = cv2.getRotationMatrix2D((s[1]/2, s[0]/2), 0, 1.1)\n",
    "    frame = cv2.warpAffine(frame, T, (s[1], s[0]))\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_black_bars(frame):\n",
    "    bw_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    up = 0\n",
    "    for i in range(bw_frame.shape[0]):\n",
    "        if np.all(bw_frame[i] == np.zeros(bw_frame[i].shape, dtype='uint8')):\n",
    "            up += 1\n",
    "        else:\n",
    "            break\n",
    "    return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 270/270 [00:16<00:00, 14.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reset stream to first frame \n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0) \n",
    "max_up = 0\n",
    "# Write n_frames-1 transformed frames\n",
    "for i in tqdm(range(n_frames-2)):\n",
    "    # Read next frame\n",
    "    success, frame = cap.read() \n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Extract transformations from the new transformation array\n",
    "    dx = transforms_smooth[i,0]\n",
    "    dy = transforms_smooth[i,1]\n",
    "    da = transforms_smooth[i,2]\n",
    "\n",
    "    # Reconstruct transformation matrix accordingly to new values\n",
    "    m = np.zeros((2,3), np.float32)\n",
    "    m[0,0] = np.cos(da)\n",
    "    m[0,1] = -np.sin(da)\n",
    "    m[1,0] = np.sin(da)\n",
    "    m[1,1] = np.cos(da)\n",
    "    m[0,2] = dx\n",
    "    m[1,2] = dy\n",
    "\n",
    "    # Apply affine wrapping to the given frame\n",
    "    frame_stabilized = cv2.warpAffine(frame, m, (w,h))\n",
    "    \n",
    "    # Fix border artifacts\n",
    "    frame_stabilized = fixBorder(frame_stabilized) \n",
    "    up = get_black_bars(frame_stabilized)\n",
    "    if up>max_up:\n",
    "        max_up = up\n",
    "    # Write the frame to the file\n",
    "    frame_out = cv2.hconcat([frame, frame_stabilized])\n",
    "\n",
    "    # If the image is too big, resize it.\n",
    "#     if(frame_out.shape[1]): \n",
    "#         frame_out = cv2.resize(frame_out, (frame_out.shape[1]//2, frame_out.shape[0]//2));\n",
    "\n",
    "#     cv2.imshow(\"Before and After\", frame_out)\n",
    "#     cv2.waitKey(ord('q'))\n",
    "    out.write(frame_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Camera capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# input video\n",
    "# cap = cv2.VideoCapture('input.avi')\n",
    "\n",
    "DIRECTORY = \"C:/Users/Seddik's PC/Desktop/Cours/OSY/Projet d'option/DepthNet_old/stereo-tracking/scenario_0\"\n",
    "\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 20.0, (1296,972))\n",
    "counter = 0\n",
    "# while(cap.isOpened()):\n",
    "for filename in tqdm(os.listdir(DIRECTORY)):\n",
    "    if filename.endswith('.jpg'):\n",
    "        counter += 1\n",
    "    #     ret, frame = cap.read()\n",
    "        frame = cv2.imread(os.path.join(DIRECTORY,filename))\n",
    "    #     print(frame.shape)\n",
    "        ret == True\n",
    "        if ret==True:\n",
    "            frame = cv2.flip(frame,0)\n",
    "\n",
    "            # write the flipped frame\n",
    "            out.write(frame)\n",
    "\n",
    "            cv2.imshow('frame',frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    #         if counter == 40:\n",
    "    #             break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
